,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,What are some examples of security measures that should be applied to evaluate and document AI system security and resilience?,"[' \n33 \nMEASURE 2.7: AI system security and resilience – as identiﬁed in the MAP function – are evaluated and documented. Action ID \nSuggested Action \nGAI Risks \nMS-2.7-001 \nApply established security measures to: Assess likelihood and magnitude of \nvulnerabilities and threats such as backdoors, compromised dependencies, data \nbreaches, eavesdropping, man-in-the-middle attacks, reverse engineering, \nautonomous agents, model theft or exposure of model weights, AI inference, \nbypass, extraction, and other baseline security concerns. Data Privacy; Information Integrity; \nInformation Security; Value Chain \nand Component Integration \nMS-2.7-002 \nBenchmark GAI system security and resilience related to content provenance \nagainst industry standards and best practices. Compare GAI system security \nfeatures and content provenance methods against industry state-of-the-art. Information Integrity; Information \nSecurity \nMS-2.7-003 \nConduct user surveys to gather user satisfaction with the AI-generated content \nand user perceptions of content authenticity. Analyze user feedback to identify \nconcerns and/or current literacy levels related to content provenance and \nunderstanding of labels on content. Human-AI Conﬁguration; \nInformation Integrity \nMS-2.7-004 \nIdentify metrics that reﬂect the eﬀectiveness of security measures, such as data \nprovenance, the number of unauthorized access attempts, inference, bypass, \nextraction, penetrations, or provenance veriﬁcation. Information Integrity; Information \nSecurity \nMS-2.7-005 \nMeasure reliability of content authentication methods, such as watermarking, \ncryptographic signatures, digital ﬁngerprints, as well as access controls, \nconformity assessment, and model integrity veriﬁcation, which can help support \nthe eﬀective implementation of content provenance techniques. Evaluate the \nrate of false positives and false negatives in content provenance, as well as true \npositives and true negatives for veriﬁcation. Information Integrity \nMS-2.7-006 \nMeasure the rate at which recommendations from security checks and incidents \nare implemented.']","Some examples of security measures that should be applied to evaluate and document AI system security and resilience include assessing the likelihood and magnitude of vulnerabilities and threats such as backdoors, compromised dependencies, data breaches, eavesdropping, man-in-the-middle attacks, reverse engineering, autonomous agents, model theft or exposure of model weights, AI inference, bypass, extraction, and other baseline security concerns. Additionally, benchmarking AI system security and resilience related to content provenance against industry standards and best practices, conducting user surveys to gather user satisfaction with AI-generated content and perceptions of content authenticity, identifying metrics reflecting the effectiveness of security measures, measuring the reliability of content authentication methods, and evaluating the rate at which recommendations from security checks and incidents are implemented.",simple,"[{'source': '../docs/NIST_AI_600-1.pdf', 'file_path': '../docs/NIST_AI_600-1.pdf', 'page': 36, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': ""D:20240805141702-04'00'"", 'modDate': ""D:20240805143048-04'00'"", 'trapped': ''}]",True
1,How can harmful bias be addressed in the GAI system to enhance transparency and accountability?,"['Confabulation \nMG-4.1-005 \nShare transparency reports with internal and external stakeholders that detail \nsteps taken to update the GAI system to enhance transparency and \naccountability. Human-AI Conﬁguration; Harmful \nBias and Homogenization \nMG-4.1-006 \nTrack dataset modiﬁcations for provenance by monitoring data deletions, \nrectiﬁcation requests, and other changes that may impact the veriﬁability of \ncontent origins. Information Integrity \n']",Harmful bias in the GAI system can be addressed to enhance transparency and accountability by implementing human-AI configuration. This involves sharing transparency reports with internal and external stakeholders detailing the steps taken to update the system for improved transparency and accountability.,simple,"[{'source': '../docs/NIST_AI_600-1.pdf', 'file_path': '../docs/NIST_AI_600-1.pdf', 'page': 47, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': ""D:20240805141702-04'00'"", 'modDate': ""D:20240805143048-04'00'"", 'trapped': ''}]",True
2,How can diverse teams improve AI red-teaming in GAI system development?,"['Small user studies can provide feedback from representative groups or \npopulations. Anonymous surveys can be used to poll or gauge reactions to speciﬁc features. Participatory \nengagement methods are often less structured than ﬁeld testing or red teaming, and are more \ncommonly used in early stages of AI or product development. Field Testing \nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions \nunder which the GAI system will be deployed. Field style tests can be adapted from a focus on user \npreferences and experiences towards AI risks and impacts – both negative and positive. When carried \nout with large groups of users, these tests can provide estimations of the likelihood of risks and impacts \nin real world interactions. Organizations may also collect feedback on outcomes, harms, and user experience directly from users in \nthe production environment after a model has been released, in accordance with human subject \nstandards such as informed consent and compensation. Organizations should follow applicable human \nsubjects research requirements, and best practices such as informed consent and subject compensation, \nwhen implementing feedback activities. AI Red-teaming \nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \nenvironment and in collaboration with AI developers building AI models to identify potential adverse \nbehavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards”. AI \nred-teaming can be performed before or after AI models or systems are made available to the broader \npublic; this section focuses on red-teaming in pre-deployment contexts. The quality of AI red-teaming outputs is related to the background and expertise of the AI red team \nitself. Demographically and interdisciplinarily diverse AI red teams can be used to identify ﬂaws in the \nvarying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results \nshould be given additional analysis before they are incorporated into organizational governance and \ndecision making, policy and procedural updates, and AI risk management eﬀorts. Various types of AI red-teaming may be appropriate, depending on the use case: \n• \nGeneral Public: Performed by general users (not necessarily AI or technical experts) who are \nexpected to use the model or interact with its outputs, and who bring their own lived \nexperiences and perspectives to the task of AI red-teaming. These individuals may have been \nprovided instructions and material to complete tasks which may elicit harmful model behaviors. This type of exercise can be more eﬀective with large groups of AI red-teamers. • \nExpert: Performed by specialists with expertise in the domain or speciﬁc AI red-teaming context \nof use (e.g., medicine, biotech, cybersecurity). • \nCombination: In scenarios when it is diﬃcult to identify and recruit specialists with suﬃcient \ndomain and contextual expertise, AI red-teaming exercises may leverage both expert and \n']","Diverse AI red teams can improve AI red-teaming in GAI system development by bringing a range of backgrounds, expertise, and perspectives to the evaluation process. Demographically and interdisciplinarily diverse teams can identify flaws in various contexts where the GAI system will be used, enhancing the quality of red-teaming outputs. For optimal results, AI red teams should demonstrate domain expertise and an understanding of socio-cultural aspects within the deployment context. The results of AI red-teaming should undergo additional analysis before being integrated into organizational governance, decision-making, policy updates, and AI risk management efforts.",multi_context,"[{'source': '../docs/NIST_AI_600-1.pdf', 'file_path': '../docs/NIST_AI_600-1.pdf', 'page': 53, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': ""D:20240805141702-04'00'"", 'modDate': ""D:20240805143048-04'00'"", 'trapped': ''}]",True
3,How can GAI risks be reduced with adversarial testing and other methods while maintaining operator skills and content transparency?,"[""Action ID \nSuggested Action \nGAI Risks \nMS-4.2-001 \nConduct adversarial testing at a regular cadence to map and measure GAI risks, \nincluding tests to address attempts to deceive or manipulate the application of \nprovenance techniques or other misuses. Identify vulnerabilities and \nunderstand potential misuse scenarios and unintended outputs. Information Integrity; Information \nSecurity \nMS-4.2-002 \nEvaluate GAI system performance in real-world scenarios to observe its \nbehavior in practical environments and reveal issues that might not surface in \ncontrolled and optimized testing environments. Human-AI Conﬁguration; \nConfabulation; Information \nSecurity \nMS-4.2-003 \nImplement interpretability and explainability methods to evaluate GAI system \ndecisions and verify alignment with intended purpose. Information Integrity; Harmful Bias \nand Homogenization \nMS-4.2-004 \nMonitor and document instances where human operators or other systems \noverride the GAI's decisions. Evaluate these cases to understand if the overrides \nare linked to issues related to content provenance. Information Integrity \nMS-4.2-005 \nVerify and document the incorporation of results of structured public feedback \nexercises into design, implementation, deployment approval (“go”/“no-go” \ndecisions), monitoring, and decommission decisions. Human-AI Conﬁguration; \nInformation Security \nAI Actor Tasks: AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV \n \n""]","Conducting adversarial testing at a regular cadence can help map and measure GAI risks, including attempts to deceive or manipulate the application of provenance techniques. This can identify vulnerabilities and potential misuse scenarios. Additionally, evaluating GAI system performance in real-world scenarios can reveal issues that may not surface in controlled testing environments. Implementing interpretability and explainability methods can help evaluate GAI system decisions and ensure alignment with intended purposes. Monitoring instances where human operators override GAI decisions can also provide insights into potential issues related to content provenance. Finally, verifying the incorporation of structured public feedback into design and decision-making processes can enhance transparency and reduce risks.",multi_context,"[{'source': '../docs/NIST_AI_600-1.pdf', 'file_path': '../docs/NIST_AI_600-1.pdf', 'page': 42, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': ""D:20240805141702-04'00'"", 'modDate': ""D:20240805143048-04'00'"", 'trapped': ''}]",True
4,What expectations should be met by automated systems in terms of data privacy?,"[' \n \n \n \n \n \nDATA PRIVACY \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. Data access and correction. People whose data is collected, used, shared, or stored by automated \nsystems should be able to access data and metadata about themselves, know who has access to this data, and \nbe able to correct it if necessary. Entities should receive consent before sharing data with other entities and \nshould keep records of what data is shared and with whom. Consent withdrawal and data deletion. Entities should allow (to the extent legally permissible) with\xad\ndrawal of data access consent, resulting in the deletion of user data, metadata, and the timely removal of \ntheir data from any systems (e.g., machine learning models) derived from that data.68\nAutomated system support. Entities designing, developing, and deploying automated systems should \nestablish and maintain the capabilities that will allow individuals to use their own automated systems to help \nthem make consent, access, and control decisions in a complex data ecosystem. Capabilities include machine \nreadable data, standardized data formats, metadata or tags for expressing data processing permissions and \npreferences and data provenance and lineage, context of use and access-specific tags, and training models for \nassessing privacy risk. Demonstrate that data privacy and user control are protected \nIndependent evaluation. As described in the section on Safe and Effective Systems, entities should allow \nindependent evaluation of the claims made regarding data policies. These independent evaluations should be \nmade public whenever possible. Care will need to be taken to balance individual privacy with evaluation data \naccess needs.']","The expectations for automated systems in terms of data privacy include allowing people to access data and metadata about themselves, correcting data if necessary, receiving consent before sharing data, keeping records of shared data, allowing withdrawal of data access consent resulting in data deletion, supporting individuals in making consent, access, and control decisions, demonstrating data privacy and user control protection, and allowing independent evaluation of data policies.",simple,"[{'source': '../docs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'file_path': '../docs/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 34, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': ""D:20220920133035-04'00'"", 'modDate': ""D:20221003104118-04'00'"", 'trapped': ''}]",True
