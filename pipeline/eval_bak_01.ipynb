{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision, answer_correctness\n",
    "import pandas as pd\n",
    "from rag_pipeline import create_rag_pipeline, create_embeddings, create_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your documents (use the same documents as the Chainlit app)\n",
    "pdf_path_1 = \"../docs/Blueprint-for-an-AI-Bill-of-Rights.pdf\"\n",
    "pdf_path_2 = \"../docs/NIST_AI_600-1.pdf\"\n",
    "loader1 = PyMuPDFLoader(pdf_path_1)\n",
    "loader2 = PyMuPDFLoader(pdf_path_2)\n",
    "documents1 = loader1.load()\n",
    "documents2 = loader2.load()\n",
    "documents = documents1 + documents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 137\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into chunks\n",
    "text_splitter_eval = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "eval_documents = text_splitter_eval.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document chunks created: 761\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of document chunks created: {len(eval_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd243c729e4b4a7780c4349a13886f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/1522 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181d6da8acd94d7392e06b031eff3837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up embeddings using ADA\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "embeddings = create_embeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# Set up the Qdrant vector store\n",
    "vectorstore = create_vector_store(eval_documents, embeddings)\n",
    "\n",
    "# Create the RAG pipeline using the current ADA model\n",
    "retriever = vectorstore.as_retriever()\n",
    "rag_pipeline = create_rag_pipeline(retriever)\n",
    "\n",
    "# Define the LLMs for test set generation\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Initialize the TestsetGenerator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Define test set distribution (simple, multi_context, reasoning)\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}\n",
    "\n",
    "# Generate synthetic test set (5 QA pairs for this example)\n",
    "num_qa_pairs = 5\n",
    "testset = generator.generate_with_langchain_docs(eval_documents, num_qa_pairs, distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated synthetic test set:\n",
      "                                            question  \\\n",
      "0  What tasks are AI Actors responsible for in th...   \n",
      "1  How does the lack of transparency in GAI syste...   \n",
      "2  How can organizations document AI risk within ...   \n",
      "3  How does generative AI impact data poisoning i...   \n",
      "4  How can AI teams ensure diverse representation...   \n",
      "\n",
      "                                            contexts  \\\n",
      "0  [Chain and Component Integration \\nMP-5.2-002 ...   \n",
      "1  [2.4. Data Privacy \\nGAI systems raise several...   \n",
      "2  [40 \\nMANAGE 1.3: Responses to the AI risks de...   \n",
      "3  [arXiv. https://arxiv.org/pdf/2310.07879 \\nLen...   \n",
      "4  [AI Actor Tasks: AI Deployment \\n \\nMAP 1.2: I...   \n",
      "\n",
      "                                        ground_truth evolution_type  \\\n",
      "0  AI Actors responsible for the integration of c...         simple   \n",
      "1  The lack of transparency in GAI system trainin...         simple   \n",
      "2  Responses to the AI risks deemed high priority...  multi_context   \n",
      "3  The answer to given question is not present in...  multi_context   \n",
      "4  Establishing and empowering interdisciplinary ...      reasoning   \n",
      "\n",
      "                                            metadata  episode_done  \n",
      "0  [{'source': '../docs/NIST_AI_600-1.pdf', 'file...          True  \n",
      "1  [{'source': '../docs/NIST_AI_600-1.pdf', 'file...          True  \n",
      "2  [{'source': '../docs/NIST_AI_600-1.pdf', 'file...          True  \n",
      "3  [{'source': '../docs/NIST_AI_600-1.pdf', 'file...          True  \n",
      "4  [{'source': '../docs/NIST_AI_600-1.pdf', 'file...          True  \n"
     ]
    }
   ],
   "source": [
    "print(\"Generated synthetic test set:\")\n",
    "print(testset.to_pandas().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test set to pandas DataFrame for inspection\n",
    "testset_df = testset.to_pandas()\n",
    "testset_df.to_csv(\"testset.csv\")\n",
    "\n",
    "# Load the test set questions and ground truth answers\n",
    "test_df = pd.read_csv(\"testset.csv\")\n",
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the answers and context lists\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Generate answers using the current RAG pipeline\n",
    "for question in test_questions:\n",
    "    # Invoke the RAG pipeline and get the response\n",
    "    response = rag_pipeline.invoke({\"query\": question})\n",
    "    \n",
    "    # Append the generated answer (content) from the response\n",
    "    answers.append(response.content)\n",
    "    \n",
    "    # Access the retrieved context separately from the response if available\n",
    "    retrieved_context = response.additional_kwargs.get(\"context\", [])\n",
    "    contexts.append([context.page_content for context in retrieved_context])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer for question 'How can AI teams ensure diverse representation for interdisciplinary collaborations?': To ensure diverse representation for interdisciplinary collaborations, AI teams can establish and empower interdisciplinary teams that reflect a wide range of capabilities, competencies, demographic groups, domain expertise, and educational backgrounds. This approach helps prioritize opportunities for interdisciplinary collaboration and ensures a diverse mix of perspectives and skills within the team.\n",
      "Retrieved context: []\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated answer for question '{question}': {response.content}\")\n",
    "print(f\"Retrieved context: {[context.page_content for context in retrieved_context]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779ca6b308b34b55b6b65ca4c74ed2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the HuggingFace Dataset for evaluation\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": test_groundtruths\n",
    "})\n",
    "\n",
    "# Evaluate using RAGAS metrics\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness\n",
    "]\n",
    "results = evaluate(response_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question contexts  \\\n",
      "0  What tasks are AI Actors responsible for in th...       []   \n",
      "1  How does the lack of transparency in GAI syste...       []   \n",
      "2  How can organizations document AI risk within ...       []   \n",
      "3  How does generative AI impact data poisoning i...       []   \n",
      "4  How can AI teams ensure diverse representation...       []   \n",
      "\n",
      "                                              answer  \\\n",
      "0  AI Actors are responsible for tasks such as AI...   \n",
      "1  The lack of transparency in GAI system trainin...   \n",
      "2  To document AI risk within their limits, organ...   \n",
      "3                                      I don't know.   \n",
      "4  To ensure diverse representation for interdisc...   \n",
      "\n",
      "                                        ground_truth  faithfulness  \\\n",
      "0  AI Actors responsible for the integration of c...           0.0   \n",
      "1  The lack of transparency in GAI system trainin...           0.0   \n",
      "2  Responses to the AI risks deemed high priority...           0.0   \n",
      "3  The answer to given question is not present in...           0.0   \n",
      "4  Establishing and empowering interdisciplinary ...           0.0   \n",
      "\n",
      "   answer_relevancy  context_recall  context_precision  answer_correctness  \n",
      "0          0.942971             0.0                0.0            0.739558  \n",
      "1          0.981845             0.0                0.0            0.744210  \n",
      "2          1.000000             0.0                0.0            0.550266  \n",
      "3          0.000000             0.0                0.0            0.195204  \n",
      "4          1.000000             0.0                0.0            0.955598  \n"
     ]
    }
   ],
   "source": [
    "# Convert results to pandas DataFrame for analysis\n",
    "results_df = results.to_pandas()\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv(\"ragas_evaluation_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
