{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain==0.2.16 langchain-community==0.2.16 langchain-core==0.2.38 langchain-experimental==0.0.65 langchain-openai==0.1.23 langchain-qdrant==0.1.4 qdrant-client==1.11.3 ragas==0.1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.11-cp38-abi3-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.11-cp38-abi3-macosx_10_9_x86_64.whl (18.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.24.11\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 137\n",
      "Number of document chunks created: 322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.                   \n",
      "Generating: 100%|██████████| 5/5 [00:43<00:00,  8.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated synthetic test set:\n",
      "                                            question  \\\n",
      "0  What are some examples of security measures th...   \n",
      "1  How can harmful bias be addressed in the GAI s...   \n",
      "2  How can diverse teams improve AI red-teaming i...   \n",
      "3  How can GAI risks be reduced with adversarial ...   \n",
      "4  What expectations should be met by automated s...   \n",
      "\n",
      "                                            contexts  \\\n",
      "0  [ \\n33 \\nMEASURE 2.7: AI system security and r...   \n",
      "1  [Confabulation \\nMG-4.1-005 \\nShare transparen...   \n",
      "2  [Small user studies can provide feedback from ...   \n",
      "3  [Action ID \\nSuggested Action \\nGAI Risks \\nMS...   \n",
      "4  [ \\n \\n \\n \\n \\n \\nDATA PRIVACY \\nWHAT SHOULD ...   \n",
      "\n",
      "                                        ground_truth evolution_type  \\\n",
      "0  Some examples of security measures that should...         simple   \n",
      "1  Harmful bias in the GAI system can be addresse...         simple   \n",
      "2  Diverse AI red teams can improve AI red-teamin...  multi_context   \n",
      "3  Conducting adversarial testing at a regular ca...  multi_context   \n",
      "4  The expectations for automated systems in term...         simple   \n",
      "\n",
      "                                            metadata  episode_done  \n",
      "0  [{'source': '../docs/NIST_AI_600-1.pdf', 'file...          True  \n",
      "1  [{'source': '../docs/NIST_AI_600-1.pdf', 'file...          True  \n",
      "2  [{'source': '../docs/NIST_AI_600-1.pdf', 'file...          True  \n",
      "3  [{'source': '../docs/NIST_AI_600-1.pdf', 'file...          True  \n",
      "4  [{'source': '../docs/Blueprint-for-an-AI-Bill-...          True  \n",
      "Generated answer for question 'What are some examples of security measures that should be applied to evaluate and document AI system security and resilience?': Some examples of security measures that should be applied to evaluate and document AI system security and resilience include:\n",
      "\n",
      "1. Applying established security measures to assess vulnerabilities and threats such as backdoors, compromised dependencies, data breaches, eavesdropping, man-in-the-middle attacks, reverse engineering, autonomous agents, model theft, AI inference, bypass, extraction, and other baseline security concerns.\n",
      "2. Benchmarking GAI system security and resilience related to content provenance against industry standards and best practices.\n",
      "3. Conducting user surveys to gather user satisfaction with AI-generated content and user perceptions of content authenticity.\n",
      "4. Identifying metrics that reflect the effectiveness of security measures, such as data provenance, unauthorized access attempts, inference, bypass, extraction, penetrations, or provenance verification.\n",
      "5. Measuring the reliability of content authentication methods, such as watermarking, cryptographic signatures, digital fingerprints, access controls, conformity assessment, and model integrity verification.\n",
      "6. Measuring the rate at which recommendations from security checks and incidents are implemented.\n",
      "\n",
      "These are some examples of security measures mentioned in the context that should be applied to evaluate and document AI system security and resilience.\n",
      "Retrieved context: []\n",
      "Generated answer for question 'How can harmful bias be addressed in the GAI system to enhance transparency and accountability?': To address harmful bias in the GAI system to enhance transparency and accountability, one approach is to implement continuous monitoring of GAI system impacts to identify whether GAI outputs are equitable across various sub-populations. Seeking active and direct feedback from affected communities via structured feedback mechanisms or red-teaming can help monitor and improve outputs. Additionally, evaluating the quality and integrity of data used in training, the provenance of AI-generated content, and defining use cases where structured human feedback exercises would be beneficial can also help address harmful bias in GAI systems.\n",
      "Retrieved context: []\n",
      "Generated answer for question 'How can diverse teams improve AI red-teaming in GAI system development?': Diverse teams can improve AI red-teaming in GAI system development by using demographically and interdisciplinarily diverse AI red teams to identify flaws in varying contexts where GAI will be used.\n",
      "Retrieved context: []\n",
      "Generated answer for question 'How can GAI risks be reduced with adversarial testing and other methods while maintaining operator skills and content transparency?': To reduce GAI risks with adversarial testing and other methods while maintaining operator skills and content transparency, the following actions can be taken:\n",
      "\n",
      "1. Conduct adversarial testing at a regular cadence to map and measure GAI risks, including tests to address attempts to deceive or manipulate the application of provenance techniques or other misuses.\n",
      "2. Evaluate GAI system performance in real-world scenarios to observe its behavior in practical environments and reveal issues that might not surface in controlled and optimized testing environments.\n",
      "3. Implement interpretability and explainability methods to evaluate GAI system decisions and verify alignment with intended purpose.\n",
      "4. Monitor and document instances where human operators or other systems override the GAI's decisions to understand if the overrides are linked to issues related to content provenance.\n",
      "5. Verify and document the incorporation of results of structured public feedback exercises into design, implementation, deployment approval, monitoring, and decommission decisions.\n",
      "\n",
      "These actions help address GAI risks while ensuring operator skills and content transparency.\n",
      "Retrieved context: []\n",
      "Generated answer for question 'What expectations should be met by automated systems in terms of data privacy?': The expectations that should be met by automated systems in terms of data privacy include built-in privacy protections, data minimization, use and collection limitations, transparency, clear mechanisms for controlling access to and use of data, privacy by design and by default, data access and correction, consent withdrawal and data deletion, automated system support, independent evaluation of data policies, and protection of the public from unchecked surveillance.\n",
      "Retrieved context: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 25/25 [00:16<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question contexts  \\\n",
      "0  What are some examples of security measures th...       []   \n",
      "1  How can harmful bias be addressed in the GAI s...       []   \n",
      "2  How can diverse teams improve AI red-teaming i...       []   \n",
      "3  How can GAI risks be reduced with adversarial ...       []   \n",
      "4  What expectations should be met by automated s...       []   \n",
      "\n",
      "                                              answer  \\\n",
      "0  Some examples of security measures that should...   \n",
      "1  To address harmful bias in the GAI system to e...   \n",
      "2  Diverse teams can improve AI red-teaming in GA...   \n",
      "3  To reduce GAI risks with adversarial testing a...   \n",
      "4  The expectations that should be met by automat...   \n",
      "\n",
      "                                        ground_truth  faithfulness  \\\n",
      "0  Some examples of security measures that should...           0.0   \n",
      "1  Harmful bias in the GAI system can be addresse...           0.0   \n",
      "2  Diverse AI red teams can improve AI red-teamin...           0.0   \n",
      "3  Conducting adversarial testing at a regular ca...           0.0   \n",
      "4  The expectations for automated systems in term...           0.0   \n",
      "\n",
      "   answer_relevancy  context_recall  context_precision  answer_correctness  \n",
      "0          0.982296             0.0                0.0            0.995131  \n",
      "1          0.949638             0.0                0.0            0.349521  \n",
      "2          1.000000             0.0                0.0            0.523480  \n",
      "3          0.939415             0.0                0.0            0.868455  \n",
      "4          0.987443             0.0                0.0            0.827398  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision, answer_correctness\n",
    "import pandas as pd\n",
    "from rag_pipeline import create_rag_pipeline, create_embeddings, create_vector_store\n",
    "from pydantic.v1 import BaseModel\n",
    "\n",
    "\n",
    "# Load your documents (use the same documents as the Chainlit app)\n",
    "pdf_path_1 = \"../docs/Blueprint-for-an-AI-Bill-of-Rights.pdf\"\n",
    "pdf_path_2 = \"../docs/NIST_AI_600-1.pdf\"\n",
    "loader1 = PyMuPDFLoader(pdf_path_1)\n",
    "loader2 = PyMuPDFLoader(pdf_path_2)\n",
    "documents1 = loader1.load()\n",
    "documents2 = loader2.load()\n",
    "documents = documents1 + documents2\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "\n",
    "# Use Semantic Chunking instead of RecursiveCharacterTextSplitter\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "semantic_chunker = SemanticChunker(embeddings)\n",
    "\n",
    "# Split the documents into semantic chunks\n",
    "semantic_documents = semantic_chunker.split_documents(documents)\n",
    "print(f\"Number of document chunks created: {len(semantic_documents)}\")\n",
    "\n",
    "# Set up embeddings using ADA\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "embeddings = create_embeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# Set up the Qdrant vector store\n",
    "vectorstore = create_vector_store(semantic_documents, embeddings)\n",
    "\n",
    "# Create the RAG pipeline using the current ADA model\n",
    "retriever = vectorstore.as_retriever()\n",
    "rag_pipeline = create_rag_pipeline(retriever)\n",
    "\n",
    "# Define the LLMs for test set generation\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Initialize the TestsetGenerator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Define test set distribution (simple, multi_context, reasoning)\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}\n",
    "\n",
    "# Generate synthetic test set (5 QA pairs for this example)\n",
    "num_qa_pairs = 5\n",
    "testset = generator.generate_with_langchain_docs(semantic_documents, num_qa_pairs, distributions)\n",
    "print(\"Generated synthetic test set:\")\n",
    "print(testset.to_pandas().head())\n",
    "\n",
    "# Convert test set to pandas DataFrame for inspection\n",
    "testset_df = testset.to_pandas()\n",
    "testset_df.to_csv(\"testset.csv\")\n",
    "\n",
    "# Load the test set questions and ground truth answers\n",
    "test_df = pd.read_csv(\"testset.csv\")\n",
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()\n",
    "\n",
    "# Prepare the answers and context lists\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Generate answers using the current RAG pipeline\n",
    "for question in test_questions:\n",
    "    try:\n",
    "        # Invoke the RAG pipeline and get the response\n",
    "        response = rag_pipeline.invoke({\"query\": question})\n",
    "        \n",
    "        # Append the generated answer (content) from the response\n",
    "        answers.append(response.content)\n",
    "        \n",
    "        # Access the retrieved context separately from the response if available\n",
    "        retrieved_context = response.additional_kwargs.get(\"context\", [])\n",
    "        contexts.append([context.page_content for context in retrieved_context])\n",
    "\n",
    "        print(f\"Generated answer for question '{question}': {response.content}\")\n",
    "        print(f\"Retrieved context: {[context.page_content for context in retrieved_context]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question '{question}': {e}\")\n",
    "\n",
    "# Create the HuggingFace Dataset for evaluation\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": test_groundtruths\n",
    "})\n",
    "\n",
    "# Evaluate using RAGAS metrics\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness\n",
    "]\n",
    "\n",
    "# Run RAGAS evaluation\n",
    "results = evaluate(response_dataset, metrics)\n",
    "\n",
    "# Convert results to pandas DataFrame for analysis\n",
    "results_df = results.to_pandas()\n",
    "print(results_df)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(\"ragas_evaluation_results_semantic.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midterm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
